{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0de8d688998391b4e340423aec176e4bbb9afb78f2320e3ca59b2d8556c4a2b46",
   "display_name": "Python 3.8.5  ('nlpenv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "de8d688998391b4e340423aec176e4bbb9afb78f2320e3ca59b2d8556c4a2b46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py7zr\n",
    "\n",
    "\n",
    "filenames = !ls 01\n",
    "\n",
    "\n",
    "# load some sample data\n",
    "for filename in filenames:\n",
    "    with py7zr.SevenZipFile('01/'+filename, 'r') as z:\n",
    "        z.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "# load some sample data\n",
    "filename = 'sample_data/twitter-coronavirus-A-2020-05-30-00-12-35-c89f265b-13f4-49fa-93c1-8ceccfe15700'\n",
    "with open(filename, 'r') as f:\n",
    "    data = json.loads('['+f.read().replace('}{','},{')+']')\n",
    "\n",
    "# remove retweets\n",
    "tweets = [tweet for tweet in data if 'retweeted_status' not in tweet]\n",
    "# keep english language tweets only\n",
    "tweets = [tweet for tweet in tweets if tweet['lang'] == 'en']\n",
    "\n",
    "\n",
    "# seperate out just tweet text for simplicity\n",
    "ttexts = [tweet['full_text'] for tweet in tweets]\n",
    "# (probably unecessary) keep an id map: tweet text index --> tweet id\n",
    "# id_map = {i: tweets[i]['id'] for i in range(len(tweets))}\n",
    "\n",
    "# remove numbers\n",
    "ttexts = [re.sub('[0-9]+', '', tweet) for tweet in ttexts]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make stemmer to pass to vectorizer\n",
    "stemmer = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "# remove stop words, tokenize, stem\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', stop_words='english', analyzer=stemmed_words, min_df=5)\n",
    "vtweets = vectorizer.fit_transform(ttexts)\n",
    "vtweets.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf = pd.DataFrame(vtweets.toarray(), columns=sorted(vectorizer.vocabulary_.keys()))\n",
    "vdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw pca graph\n",
    "pca = PCA()\n",
    "pca.fit(vdf)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_[:300]))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply suitable dim reduciton: 300 pc\n",
    "pca = PCA(2)\n",
    "pctweets = pca.fit_transform(vdf)\n",
    "pctweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster\n",
    "# cls = MiniBatchKMeans(n_clusters=5, random_state=0)\n",
    "# cls.fit(pctweets)\n",
    "# cls.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1st 2 components\n",
    "# reduce the features to 2D\n",
    "# pca = PCA(2)\n",
    "# reduced_features = pca.fit_transform(pctweets)\n",
    "\n",
    "# # reduce the cluster centers to 2D\n",
    "# reduced_cluster_centers = pca.transform(cls.cluster_centers_)\n",
    "\n",
    "# plt.scatter(reduced_features[:,0], reduced_features[:,1], c=cls.predict(pctweets))\n",
    "# plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster\n",
    "# a paper on twitter clusttering with dbscan uses minSaples in a range of 10 to 60. I will try with 10, but it's a shot in the dark as I have no domain knowledge about what twitter text is like.\n",
    "# one tutorial indicates that minSamples hould be greater than the dimensionality of the data, but with 300 features this seems absurd\n",
    "\n",
    "# finding a good epsilon\n",
    "\n",
    "# plt.scatter(reduced_features[:,0], reduced_features[:,1], c=cls.predict(pctweets))\n",
    "# plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='b')\n",
    "neighbors = NearestNeighbors(n_neighbors=4) # I'm using minSamples=10\n",
    "neighbors_fit = neighbors.fit(pctweets)\n",
    "distances, indices = neighbors_fit.kneighbors(pctweets)\n",
    "\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances[1490:1510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this plot doesn't look right. Why are there so many distances of 0? is this even possible?\n",
    "# distances[:10]\n",
    "# np.sum(vdf.duplicated())\n",
    "#  there are nearly 100 duplicates after tokenizing the tweets. this would explain the previous plot\n",
    "# why are there so many duplicates???\n",
    "\n",
    "np.sum(pd.DataFrame(ttexts).duplicated())\n",
    "# As far as I can tell, there are no duplicate tweets, but the vector repr of some must be identical\n",
    "\n",
    "\n",
    "# according to the graph, a good eps would be 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster\n",
    "cls = DBSCAN(eps=.29, min_samples=4)\n",
    "cls.fit(pctweets)\n",
    "set(cls.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1st 2 components\n",
    "# reduce the features to 2D\n",
    "# pca = PCA(2)\n",
    "# reduced_features = pca.fit_transform(pctweets)\n",
    "reduced_features = pctweets\n",
    "\n",
    "# # reduce the cluster centers to 2D\n",
    "# reduced_cluster_centers = pca.transform(cls.cluster_centers_)\n",
    "\n",
    "plt.scatter(reduced_features[:,0], reduced_features[:,1], c=cls.fit_predict(pctweets))\n",
    "# plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try detecting negative asian sentiment within each cluster"
   ]
  }
 ]
}