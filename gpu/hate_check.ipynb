{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from hatebase import HatebaseAPI\n",
    "key = 'hTjz4vdtftaofzHEk7sdthudipaqAnzz'\n",
    "hatebase = HatebaseAPI({\"key\": key})\n",
    "# for more details, set debug to True\n",
    "hatebase = HatebaseAPI({\"key\": key, \"debug\": True})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "response text: {\n",
      "    \"version\": \"4.4\",\n",
      "    \"datetime\": \"2021-08-05 11:26:54\",\n",
      "    \"important\": \"Your use of the Hatebase API acknowledges your consent with Hatebase's Terms of Use (available online at hatebase.org) and PROHIBITS REDISTRIBUTION of this data for any purpose, including but not limited to republication in any form, such as in PUBLIC CODE REPOSITORIES.\",\n",
      "    \"query\": {\n",
      "        \"api_key\": \"hTjz4vdtftaofzHEk7sdthudipaqAnzz\"\n",
      "    },\n",
      "    \"result\": {\n",
      "        \"token\": \"uZEDVotZbpsYwkUmdJEruBdTqnqavmXp\",\n",
      "        \"expires_on\": \"2021-08-05 12:26:54\"\n",
      "    }\n",
      "}\n",
      "token: uZEDVotZbpsYwkUmdJEruBdTqnqavmXp\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# set filters for vocabulary query\n",
    "filters = [\n",
    "            {\"language\": \"eng\", \"ethnicity\": \"asian\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"vn\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"cn\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"jp\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"kr\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"ph\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"kh\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"my\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"in\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"id\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"mm\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"np\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"th\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"la\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"bd\"},\n",
    "            {\"language\": \"eng\", \"nationality\": \"lk\"}\n",
    "]\n",
    "# filters = {\"keyword\": \"mon\", \"language\": \"eng\"}\n",
    "res_format = \"json\"\n",
    "terms = {}\n",
    "reslist = []\n",
    "for filt in filters:\n",
    "    response = hatebase.getVocabulary(filters=filt, format=res_format)\n",
    "    reslist.append(response)\n",
    "    \n",
    "    if 'nationality' in filt:\n",
    "        terms[filt['nationality']] = [res['term'] for res in response['result'] if not res['nonhateful_meaning']]\n",
    "    else:\n",
    "        terms['asia'] = [res['term'] for res in response['result'] if not res['nonhateful_meaning']]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&ethnicity=asian\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=vn\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=cn\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=jp\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=kr\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=ph\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=kh\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=my\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=in\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=id\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=mm\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=np\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=th\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=la\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=bd\n",
      "response: <Response [200]>\n",
      "url: https://api.hatebase.org/4-4/get_vocabulary\n",
      "payload: token=uZEDVotZbpsYwkUmdJEruBdTqnqavmXp&format=json&language=eng&nationality=lk\n",
      "response: <Response [200]>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "terms"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'asia': ['mongoloids',\n",
       "  'mongoloid',\n",
       "  'whoriental',\n",
       "  'whorientals',\n",
       "  'gooky eyes',\n",
       "  'gooklets',\n",
       "  'gooklet',\n",
       "  'gookettes',\n",
       "  'gookette',\n",
       "  'gook eyed',\n",
       "  'gookies',\n",
       "  'gookie',\n",
       "  'goloids',\n",
       "  'goloid',\n",
       "  'ginks',\n",
       "  'gink',\n",
       "  'dog eaters',\n",
       "  'dog eater',\n",
       "  'yellow invaders',\n",
       "  'rice niggers',\n",
       "  'yellow invader',\n",
       "  'rice nigger'],\n",
       " 'vn': ['gookie',\n",
       "  'dinkladies',\n",
       "  'dinklady',\n",
       "  'Charly',\n",
       "  'Charlies',\n",
       "  'dinks',\n",
       "  'dink',\n",
       "  'tunnel diggers',\n",
       "  'mungs',\n",
       "  'tunnel digger',\n",
       "  'mung'],\n",
       " 'cn': ['spink',\n",
       "  'sideways vaginas',\n",
       "  'sideways cooters',\n",
       "  'sideways pussies',\n",
       "  'chonkies',\n",
       "  'Chinamen',\n",
       "  'chinigs',\n",
       "  'chink a billies',\n",
       "  'bamboo coons',\n",
       "  'chinig',\n",
       "  'sideways cooter',\n",
       "  'chink a billy',\n",
       "  'bamboo coon',\n",
       "  'chonky',\n",
       "  'sideways pussy',\n",
       "  'sideways vagina',\n",
       "  'Chinaman'],\n",
       " 'jp': ['yellow cabs',\n",
       "  'yellow cab',\n",
       "  'Japs',\n",
       "  'Japland',\n",
       "  'butterheads',\n",
       "  'butterhead',\n",
       "  'Jap'],\n",
       " 'kr': ['bucketheads', 'buckethead'],\n",
       " 'ph': ['filthypinos',\n",
       "  'filthypinoes',\n",
       "  'filthypino',\n",
       "  'flipettes',\n",
       "  'flipette',\n",
       "  'flipabeanos',\n",
       "  'Chinese wetbacks',\n",
       "  'buk buks',\n",
       "  'book books',\n",
       "  'flipabeano',\n",
       "  'buk buk',\n",
       "  'Chinese wetback',\n",
       "  'book book'],\n",
       " 'kh': [],\n",
       " 'my': [],\n",
       " 'in': ['dot heads',\n",
       "  'Madrasi',\n",
       "  'Biharis',\n",
       "  'dot head',\n",
       "  'cow kissers',\n",
       "  'bhremptis',\n",
       "  'bhrempti',\n",
       "  'cow kisser'],\n",
       " 'id': [],\n",
       " 'mm': [],\n",
       " 'np': [],\n",
       " 'th': [],\n",
       " 'la': [],\n",
       " 'bd': [],\n",
       " 'lk': []}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "keys = list(terms.keys())\n",
    "for key in keys:\n",
    "    if len(terms[key]) < 5:\n",
    "        terms.pop(key)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import json\n",
    "with open('hate_terms.json', 'w') as f:\n",
    "    f.write(json.dumps(terms))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "with open('week0_distribution.p', 'rb') as f:\n",
    "    dist = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "any([term in dist[0] for term in terms])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "total_topics = 20\n",
    "scores = [dist[i]['chinesevirus'] for i in range(total_topics)]\n",
    "\n",
    "plt.bar(range(total_topics), scores)\n",
    "plt.xticks(range(total_topics))\n",
    "plt.ylabel('probability')\n",
    "plt.xlabel('topic number')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('nlpenv': venv)"
  },
  "interpreter": {
   "hash": "de8d688998391b4e340423aec176e4bbb9afb78f2320e3ca59b2d8556c4a2b46"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}