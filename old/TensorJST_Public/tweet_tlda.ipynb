{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import gamma\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.cp_tensor import cp_mode_dot\n",
    "import tensorly.tenalg as tnl\n",
    "from tensorly.tenalg.core_tenalg import tensor_dot, batched_tensor_dot, outer, inner\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from pca import PCA\n",
    "\n",
    "# Import TensorLy\n",
    "import tensorly as tl\n",
    "from tensorly.tenalg import kronecker\n",
    "from tensorly import norm\n",
    "from tensorly.decomposition import symmetric_parafac_power_iteration as sym_parafac\n",
    "from tensorly.tenalg.core_tenalg.tensor_product import batched_tensor_dot\n",
    "from tensorly.testing import assert_array_equal, assert_array_almost_equal\n",
    "\n",
    "from tensorly.contrib.sparse.cp_tensor import cp_to_tensor\n",
    "\n",
    "from tlda_final import TLDA\n",
    "import cumulant_gradient\n",
    "import tensor_lda_util as tl_util\n",
    "## Break down into steps, then re-engineer.\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        to_return = [porter.stem(self.wnl.lemmatize(t,get_wordnet_pos(t))) for t in word_tokenize(articles)]\n",
    "        return to_return\n",
    "    \n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.porter.stem(t) for t in word_tokenize(articles)]\n",
    "\n",
    "from helpers import gtp \n",
    "\n",
    "        \n",
    "import gc\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import scipy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/adrien/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/adrien/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/adrien/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "!echo $PWD"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/adrien/twitter-analysis/old/TensorJST_Public\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# load tweets\n",
    "\n",
    "import os\n",
    "from helpers import load_tweets\n",
    "filenames = ['../../data/unzipped/' + name for name in os.listdir('../../data/unzipped')]\n",
    "\n",
    "all_tweets = load_tweets(filenames, preprocessor=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "len(all_tweets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "331755"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# use only 100000 tweets\n",
    "tweets = random.sample(all_tweets, 100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# def stop words\n",
    "n_samples = len(tweets)\n",
    "print(n_samples)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "gtp('#chinesevirus')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['chinesevirus']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "vocab = list(np.load('../../bigVocabCountVec_id-word-map_2021-07-12.npy')) + gtp('#chinesevirus')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# countvec = CountVectorizer(tokenizer=gtp,\n",
    "countvec = CountVectorizer(tokenizer=gtp,\n",
    "                                strip_accents = 'unicode', # works\n",
    "                                lowercase = True, # works\n",
    "                                ngram_range = (1,2),\n",
    "                                # max_df = 0.4, # works\n",
    "                                # min_df = int(0.002*n_samples))\n",
    "                                vocabulary=vocab)\n",
    "\n",
    "dtm = countvec.fit_transform(tweets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "dtm_sent = scipy.sparse.csr_matrix(dtm)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "a = tl.tensor(dtm_sent.toarray(),dtype=np.float16)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "del dtm_sent\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10429"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "M1      = tl.mean(a, axis=0)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "M1.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1001,)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "x_cent = scipy.sparse.csr_matrix(a - M1,dtype=np.float16) #center the data using the first moment \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "gc.collect()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "\n",
    "start = datetime.now()\n",
    "print(\"now =\", start)\n",
    "\n",
    "\n",
    "batch_size = int(n_samples/20)\n",
    "verbose = True\n",
    "n_topic =  20\n",
    "\n",
    "beta_0=0.003\n",
    "\n",
    "pca = PCA(n_topic, beta_0, 30000)\n",
    "pca.fit(x_cent) # fits PCA to  data, gives W\n",
    "x_whit = pca.transform(x_cent) # produces a whitened words counts <W,x> for centered data x\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "pca_time = now - start \n",
    "\n",
    "'''\n",
    "for data in yield_data:\n",
    "    pca.partial_fit(data)\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "now = 2021-07-12 17:58:06.839960\n",
      "now = 2021-07-12 17:58:34.728216\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nfor data in yield_data:\\n    pca.partial_fit(data)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "gc.collect()\n",
    "print(pca_time)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:00:27.888256\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from importlib import reload  \n",
    "import tlda_final\n",
    "reload(tlda_final)\n",
    "from tlda_final import TLDA\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "learning_rate = 0.01 \n",
    "batch_size =15000\n",
    "t = TLDA(n_topic,n_senti=1, alpha_0= beta_0, n_iter_train=1000, n_iter_test=150, batch_size=batch_size,\n",
    "         learning_rate=learning_rate)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "now = 2021-07-12 17:58:35.053658\n",
      "now = 2021-07-12 17:58:35.056311\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "\n",
    "t.fit(x_whit,verbose=True) # fit whitened wordcounts to get decomposition of M3 through SGD\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "now = 2021-07-12 17:58:35.087977\n",
      "Epoch: 200\n",
      "Epoch: 400\n",
      "Epoch: 600\n",
      "Epoch: 800\n",
      "now = 2021-07-12 17:58:58.259117\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "\n",
    "\n",
    "factors = pca.reverse_transform(t.factors_)  # unwhiten the eigenvectors to get unscaled word-level factors\n",
    "t.factors_ = factors\n",
    "''' \n",
    "Recover alpha_hat from the eigenvalues of M3\n",
    "'''  \n",
    "\n",
    "eig_vals = [np.linalg.norm(k,3) for k in factors ]\n",
    "# normalize beta\n",
    "alpha      = np.power(eig_vals, -2)\n",
    "print(alpha.shape)\n",
    "alpha_norm = (alpha / alpha.sum()) * beta_0\n",
    "t.alpha_   = alpha_norm\n",
    "        \n",
    "print(alpha_norm)\n",
    "\n",
    "t.predict(x_whit,w_mat=True,doc_predict=False)  # normalize the factors \n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "now = 2021-07-12 17:58:58.321296\n",
      "(20,)\n",
      "[1.21965117e-04 2.16296231e-04 9.24462428e-05 1.00529733e-04\n",
      " 1.20209706e-04 2.32833776e-04 2.01127262e-04 1.13486933e-04\n",
      " 1.36614986e-04 7.70022425e-05 1.39274383e-04 2.83587408e-04\n",
      " 9.71779783e-05 2.12841560e-04 2.51512292e-04 7.77996945e-05\n",
      " 9.28144477e-05 1.27570119e-04 2.29225037e-04 7.56848520e-05]\n",
      "now = 2021-07-12 17:58:58.333220\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "id_map = np.array(countvec.get_feature_names())\n",
    "np.save('id_map.npy', id_map)\n",
    "np.save('factors.npy', factors)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "import pickle\n",
    "with open('trained_tlda.pickle', 'wb') as f:\n",
    "    pickle.dump(t, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "with open('trained_tlda.pickle', 'rb') as f:\n",
    "    t = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "t.factors_.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20, 1001)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# top = id_map[factors[0,:].argsort()[-20:]]\n",
    "top_words = factors[0,:].argsort()\n",
    "'chinesevirus' in id_map[top_words]\n",
    "np.where(id_map == 'chinesevirus')\n",
    "id_map[-1]\n",
    "stf = []\n",
    "for i in range(n_topic):\n",
    "    stf.append(factors[i,1000])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(n_topic), stf)\n",
    "plt.xticks(ticks=range(20))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f49f221ec40>,\n",
       "  <matplotlib.axis.XTick at 0x7f49f221ecd0>,\n",
       "  <matplotlib.axis.XTick at 0x7f49e264cb20>,\n",
       "  <matplotlib.axis.XTick at 0x7f49e06c4040>,\n",
       "  <matplotlib.axis.XTick at 0x7f49e06c4b20>,\n",
       "  <matplotlib.axis.XTick at 0x7f49e06c4cd0>,\n",
       "  <matplotlib.axis.XTick at 0x7f4a0f03aee0>,\n",
       "  <matplotlib.axis.XTick at 0x7f49f11ba640>,\n",
       "  <matplotlib.axis.XTick at 0x7f49f11bab80>,\n",
       "  <matplotlib.axis.XTick at 0x7f49f11baeb0>,\n",
       "  <matplotlib.axis.XTick at 0x7f49f11b1af0>,\n",
       "  <matplotlib.axis.XTick at 0x7f49f11b1550>,\n",
       "  <matplotlib.axis.XTick at 0x7f49f11b1b50>,\n",
       "  <matplotlib.axis.XTick at 0x7f49f11bad60>,\n",
       "  <matplotlib.axis.XTick at 0x7f49dde17bb0>,\n",
       "  <matplotlib.axis.XTick at 0x7f49dde17160>,\n",
       "  <matplotlib.axis.XTick at 0x7f49dde0c7c0>,\n",
       "  <matplotlib.axis.XTick at 0x7f49dde0c0a0>,\n",
       "  <matplotlib.axis.XTick at 0x7f4a0f0498e0>,\n",
       "  <matplotlib.axis.XTick at 0x7f49dde0c280>],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "metadata": {},
     "execution_count": 71
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# top = factors[0,factors[0,:].argsort()[-20:]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "n_top_words=20\n",
    "#print(t_n_indices)\n",
    "n_sentiments = 1\n",
    "top_words_JST = None\n",
    "for k in range(n_topic*n_sentiments):\n",
    "    if k ==0:\n",
    "        t_n_indices   =factors[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_JST = [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]\n",
    "    else:\n",
    "        t_n_indices   =factors[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_JST = np.vstack([top_words_JST, [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]])\n",
    "        # print([i for i,v in countvec.vocabulary_.items() if v in t_n_indices])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "sorted(top) == sorted(top_words_JST[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "top_words_JST"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['activ', 'case', 'case coronavirus', 'case death', 'case http',\n",
       "        'china', 'communist', 'confirm', 'confirm case',\n",
       "        'coronavirus case', 'covid case', 'georgia', 'hell', 'hide',\n",
       "        'know', 'probabl', 'record', 'total', 'true', 'truth'],\n",
       "       ['bank', 'campaign', 'congress', 'cost', 'donat', 'econom',\n",
       "        'financi', 'fund', 'help', 'leader', 'leadership', 'loan',\n",
       "        'local', 'need', 'option', 'pelosi', 'prepar', 'student',\n",
       "        'suppli', 'support'],\n",
       "       ['agenc', 'author', 'canada', 'china', 'claim', 'counti', 'cover',\n",
       "        'covid case', 'covid patient', 'critic', 'detail', 'inform',\n",
       "        'investig', 'origin', 'outbreak', 'outbreak http', 'patient',\n",
       "        'trade', 'travel', 'wuhan'],\n",
       "       ['amid coronavirus', 'bring', 'close', 'coronavirus outbreak',\n",
       "        'despit', 'drop', 'forc', 'live', 'live updat', 'offic',\n",
       "        'offici', 'outbreak', 'past', 'plan', 'report', 'return', 'see',\n",
       "        'warn', 'world', 'wuhan'],\n",
       "       ['china', 'come', 'communist', 'contain', 'countri', 'econom',\n",
       "        'europ', 'gate', 'global', 'govt', 'http http', 'impact', 'know',\n",
       "        'market', 'measur', 'read', 'resourc', 'start', 'truth', 'world'],\n",
       "       ['biden', 'corona', 'corona virus', 'corrupt', 'dead', 'destroy',\n",
       "        'disinfect', 'donald', 'fake', 'fake news', 'inject',\n",
       "        'inject disinfect', 'mental', 'polit', 'push', 'virus http',\n",
       "        'virus kill', 'vote', 'white', 'white hous'],\n",
       "       ['appreci', 'celebr', 'challeng', 'compani', 'crisi', 'employ',\n",
       "        'essenti', 'frontlin', 'great', 'happi', 'hard', 'hero', 'line',\n",
       "        'organ', 'safeti', 'servic', 'share', 'thank', 'work', 'worker'],\n",
       "       ['agre', 'come', 'contact', 'go', 'good', 'happen', 'hear',\n",
       "        'instead', 'kid', 'later', 'look', 'need', 'probabl', 'rest',\n",
       "        'run', 'say', 'sure', 'thing', 'true', 'want'],\n",
       "       ['african', 'bori', 'coronavirus lockdown', 'covid lockdown',\n",
       "        'download', 'eas', 'franc', 'india', 'indian', 'johnson', 'lift',\n",
       "        'lockdown', 'lockdown http', 'news http', 'restrict', 'rule',\n",
       "        'sweden', 'time', 'time http', 'york'],\n",
       "       ['agre', 'busi', 'donat', 'educ', 'hear', 'help', 'incom', 'kid',\n",
       "        'listen', 'longer', 'love', 'money', 'need', 'right', 'small',\n",
       "        'soon', 'student', 'thing', 'understand', 'wear'],\n",
       "       ['anim', 'babi', 'cancel', 'definit', 'earth', 'game', 'human',\n",
       "        'immun', 'littl', 'long', 'mayb', 'natur', 'phone', 'play',\n",
       "        'season', 'shit', 'sound', 'summer', 'twitter', 'year'],\n",
       "       ['actual', 'cancer', 'corona', 'corona virus', 'cure', 'damn',\n",
       "        'death rate', 'differ', 'heart', 'herd', 'immun', 'mean',\n",
       "        'mortal', 'piersmorgan', 'quarantin', 'rate', 'sweden', 'till',\n",
       "        'type', 'wonder'],\n",
       "       ['activ', 'africa', 'articl', 'case', 'case covid', 'case http',\n",
       "        'confirm', 'counti', 'covid case', 'data', 'depart', 'georgia',\n",
       "        'know', 'korea', 'nigeria', 'north', 'reopen', 'south', 'state',\n",
       "        'unit state'],\n",
       "       ['coronavirus covid_', 'coronavirus http', 'coronavirus vaccin',\n",
       "        'covid coronavirus', 'covid_ coronavirus', 'depress', 'develop',\n",
       "        'feel', 'futur', 'great', 'hard', 'life', 'like', 'look like',\n",
       "        'sar', 'sound', 'special', 'thank', 'vaccin', 'work'],\n",
       "       ['contact', 'coronavirus lockdown', 'coronavirus test', 'eas',\n",
       "        'free', 'indian', 'kit', 'monday', 'offer', 'open', 'plant',\n",
       "        'polic', 'posit coronavirus', 'quarantin', 'south', 'state',\n",
       "        'test', 'test posit', 'trace', 'train'],\n",
       "       ['anim', 'call', 'chines', 'conspiraci', 'control', 'corona',\n",
       "        'covid virus', 'creat', 'cure', 'danger', 'exist', 'fact',\n",
       "        'fauci', 'hoax', 'human', 'novel', 'origin', 'person',\n",
       "        'scientist', 'youtub'],\n",
       "       ['amid covid', 'avail', 'center', 'communiti', 'covid',\n",
       "        'covid http', 'covid outbreak', 'covid patient', 'covid test',\n",
       "        'current', 'employe', 'fight covid', 'impact covid', 'member',\n",
       "        'patient', 'posit covid', 'post', 'resid', 'senat', 'staff'],\n",
       "       ['best', 'decis', 'demand', 'give', 'middl', 'middl pandem',\n",
       "        'move', 'onlin', 'opportun', 'period', 'plan', 'price', 'secur',\n",
       "        'space', 'spend', 'take', 'time', 'time http', 'video', 'watch'],\n",
       "       ['biden', 'blood', 'brief', 'campaign', 'comment', 'cover',\n",
       "        'disinfect', 'donald', 'donald trump', 'find', 'golf', 'mask',\n",
       "        'penc', 'presid trump', 'stori', 'support', 'trump',\n",
       "        'trump coronavirus', 'white', 'white hous'],\n",
       "       ['allow', 'avoid', 'beach', 'california', 'coronavirus crisi',\n",
       "        'coronavirus outbreak', 'cuomo', 'distanc', 'emerg',\n",
       "        'fight coronavirus', 'governor', 'healthi', 'rule', 'social',\n",
       "        'social distanc', 'spread', 'spread coronavirus', 'spread virus',\n",
       "        'stop', 'urg']], dtype='<U20')"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "now = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "print(\"now =\", now)\n",
    "print(t.factors_.shape)\n",
    "a_word       = tl.tensor(dtm.toarray(),dtype=tl.float32)\n",
    "\n",
    "doc_topic_dist, topic_word_dist = t.predict(a_word,w_mat=False,doc_predict=True)\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "end = datetime.now()\n",
    "print(end)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "now = 2021-07-12 17:58:58.789043\n",
      "(20, 1001)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-0cda25b25295>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ma_word\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_word_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_mat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/twitter-analysis/old/TensorJST_Public/tlda_final.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X_test, w_mat, doc_predict)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc_predict\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mgammad_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mgammad_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/twitter-analysis/old/TensorJST_Public/tlda_final.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc_predict\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mgammad_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mgammad_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/twitter-analysis/old/TensorJST_Public/tlda_final.py\u001b[0m in \u001b[0;36m_predict_topic\u001b[0;34m(self, doc, adjusted_factor, w_mat)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmean_gamma_change\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlastgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mgammad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_elogthetad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_elogbetad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mphinorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mexp_elogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mphinorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_elogbetad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_elogthetad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de8d688998391b4e340423aec176e4bbb9afb78f2320e3ca59b2d8556c4a2b46"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('nlpenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}