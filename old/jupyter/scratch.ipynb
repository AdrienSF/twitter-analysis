{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0de8d688998391b4e340423aec176e4bbb9afb78f2320e3ca59b2d8556c4a2b46",
   "display_name": "Python 3.8.5  ('nlpenv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "de8d688998391b4e340423aec176e4bbb9afb78f2320e3ca59b2d8556c4a2b46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/adrien/twitter-analysis/nlpenv/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "def compute_coherence_values(corpus, dictionary, k, a='symmetric', b=None, coherence='u_mass', texts=None):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b,\n",
    "                                           workers=10)\n",
    "    \n",
    "    if coherence == 'u_mass':\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, corpus=corpus, coherence=coherence, processes=4)\n",
    "    else:\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, coherence=coherence, processes=4)\n",
    "\n",
    "    # print('coherence: ', coherence_model_lda.get_coherence())\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_stemming(text, stemmer, lemmatizer):\n",
    "    # return SnowballStemmer('english').stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "    return stemmer.stem(lemmatizer.lemmatize(text, pos='v'))\n",
    "\n",
    "def get_preprocessed(text, stemmer, lemmatizer):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3: #[NOTE]: maybe should relax this restriction of >3\n",
    "            result.append(lemmatize_stemming(token, stemmer, lemmatizer))\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_tweets(filenames, preprocess=False):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    all_tweets = []\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r') as f:\n",
    "            # add commas between tweets to correct json syntax\n",
    "            data = json.loads('['+f.read().replace('}{','},{')+']')\n",
    "        # remove retweets\n",
    "        tweets = [tweet for tweet in data if 'retweeted_status' not in tweet]\n",
    "        # keep english language tweets only\n",
    "        tweets = [tweet for tweet in tweets if tweet['lang'] == 'en']\n",
    "\n",
    "        # take tweet text  or full_text if the tweet has that attribute\n",
    "        if preprocess:\n",
    "            ttexts = [ get_preprocessed(tweet['extended_tweet']['full_text'], stemmer, lemmatizer) if 'full_text' in tweet else get_preprocessed(tweet['text'], stemmer, lemmatizer) for tweet in tweets]\n",
    "        else:\n",
    "            ttexts = [ tweet['extended_tweet']['full_text'] if 'full_text' in tweet else tweet['text'] for tweet in tweets]\n",
    "\n",
    "\n",
    "        all_tweets = all_tweets + ttexts\n",
    "\n",
    "\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "filenames = ['data/unzipped/' + name for name in os.listdir('data/unzipped')]\n",
    "\n",
    "filenames = filenames[:4]\n",
    "\n",
    "all_tweets = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        # add commas between tweets to correct json syntax\n",
    "        data = json.loads('['+f.read().replace('}{','},{')+']')\n",
    "    # remove retweets\n",
    "    tweets = [tweet for tweet in data if 'retweeted_status' not in tweet]\n",
    "    # keep english language tweets only\n",
    "    tweets = [tweet for tweet in tweets if tweet['lang'] == 'en']\n",
    "\n",
    "    # take tweet text  or full_text if the tweet has that attribute\n",
    "    ttexts = []\n",
    "    for tweet in tweets:\n",
    "        if 'full_text' in tweet:\n",
    "            ttexts.append(tweet['full_text'])\n",
    "        elif 'extended_tweet' in tweet and 'full_text' in tweet['extended_tweet']:\n",
    "            ttexts.append(tweet['extended_tweet']['full_text'])\n",
    "        else:\n",
    "            ttexts.append(tweet['text'])\n",
    "\n",
    "    all_tweets = all_tweets + tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7818"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "len([tweet for tweet in all_tweets if 'text' in tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10481"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# all: 10481\n",
    "# full_text in tweet: 2663\n",
    "# text in tw: 7818\n",
    "# extended in tw: 4351\n",
    "# all extendeds have full text\n",
    "2663 + 7818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}