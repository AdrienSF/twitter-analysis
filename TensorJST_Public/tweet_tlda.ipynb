{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/adrien/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/adrien/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/adrien/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import gamma\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.cp_tensor import cp_mode_dot\n",
    "import tensorly.tenalg as tnl\n",
    "from tensorly.tenalg.core_tenalg import tensor_dot, batched_tensor_dot, outer, inner\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from pca import PCA\n",
    "\n",
    "# Import TensorLy\n",
    "import tensorly as tl\n",
    "from tensorly.tenalg import kronecker\n",
    "from tensorly import norm\n",
    "from tensorly.decomposition import symmetric_parafac_power_iteration as sym_parafac\n",
    "from tensorly.tenalg.core_tenalg.tensor_product import batched_tensor_dot\n",
    "from tensorly.testing import assert_array_equal, assert_array_almost_equal\n",
    "\n",
    "from tensorly.contrib.sparse.cp_tensor import cp_to_tensor\n",
    "\n",
    "from tlda_final import TLDA\n",
    "import cumulant_gradient\n",
    "import tensor_lda_util as tl_util\n",
    "## Break down into steps, then re-engineer.\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        to_return = [porter.stem(self.wnl.lemmatize(t,get_wordnet_pos(t))) for t in word_tokenize(articles)]\n",
    "        return to_return\n",
    "    \n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.porter.stem(t) for t in word_tokenize(articles)]\n",
    "\n",
    "from helpers import gtp \n",
    "\n",
    "        \n",
    "import gc\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tweets\n",
    "\n",
    "import os\n",
    "from helpers import load_tweets\n",
    "filenames = ['../data/unzipped/' + name for name in os.listdir('../data/unzipped')]\n",
    "\n",
    "all_tweets = load_tweets(filenames, preprocessor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331755"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only 100000 tweets\n",
    "tweets = random.sample(all_tweets, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# def stop words\n",
    "n_samples = len(tweets)\n",
    "print(n_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countvec = CountVectorizer(tokenizer=gtp,\n",
    "countvec = CountVectorizer(tokenizer=gtp,\n",
    "                                strip_accents = 'unicode', # works\n",
    "                                lowercase = True, # works\n",
    "                                ngram_range = (1,2),\n",
    "                                max_df = 0.4, # works\n",
    "                                min_df = int(0.002*n_samples))\n",
    "\n",
    "dtm = countvec.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_sent = scipy.sparse.csr_matrix(dtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tl.tensor(dtm_sent.toarray(),dtype=np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dtm_sent\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1      = tl.mean(a, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1094,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cent = scipy.sparse.csr_matrix(a - M1,dtype=np.float16) #center the data using the first moment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-07-06 14:37:35.244170\n",
      "now = 2021-07-06 14:38:15.656016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = datetime.now()\n",
    "print(\"now =\", start)\n",
    "\n",
    "\n",
    "batch_size = int(n_samples/20)\n",
    "verbose = True\n",
    "n_topic =  20\n",
    "\n",
    "beta_0=0.003\n",
    "\n",
    "pca = PCA(n_topic, beta_0, 30000)\n",
    "pca.fit(x_cent) # fits PCA to  data, gives W\n",
    "x_whit = pca.transform(x_cent) # produces a whitened words counts <W,x> for centered data x\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "pca_time = now - start \n",
    "\n",
    "'''\n",
    "for data in yield_data:\n",
    "    pca.partial_fit(data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:40.411846\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(pca_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-07-06 14:38:15.909869\n",
      "now = 2021-07-06 14:38:15.912181\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload  \n",
    "import tlda_final\n",
    "reload(tlda_final)\n",
    "from tlda_final import TLDA\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "learning_rate = 0.01 \n",
    "batch_size =15000\n",
    "t = TLDA(n_topic,n_senti=1, alpha_0= beta_0, n_iter_train=1000, n_iter_test=150, batch_size=batch_size,\n",
    "         learning_rate=learning_rate)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-07-06 14:38:15.951018\n",
      "Epoch: 200\n",
      "Epoch: 400\n",
      "Epoch: 600\n",
      "Epoch: 800\n",
      "now = 2021-07-06 14:38:37.196878\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "\n",
    "t.fit(x_whit,verbose=True) # fit whitened wordcounts to get decomposition of M3 through SGD\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-07-06 14:38:37.251422\n",
      "(20,)\n",
      "[1.94053055e-04 1.40863567e-04 1.23552553e-04 5.36131662e-05\n",
      " 9.56070175e-05 1.25305461e-04 1.48062691e-04 2.32165629e-04\n",
      " 2.19216766e-04 1.50064902e-04 2.84439743e-04 8.73546713e-05\n",
      " 8.58187594e-05 7.12379348e-05 1.65352424e-04 1.36128575e-04\n",
      " 8.20461080e-05 4.00936039e-04 1.29579522e-04 7.46014169e-05]\n",
      "now = 2021-07-06 14:38:37.256069\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "\n",
    "\n",
    "factors = pca.reverse_transform(t.factors_)  # unwhiten the eigenvectors to get unscaled word-level factors\n",
    "t.factors_ = factors\n",
    "''' \n",
    "Recover alpha_hat from the eigenvalues of M3\n",
    "'''  \n",
    "\n",
    "eig_vals = [np.linalg.norm(k,3) for k in factors ]\n",
    "# normalize beta\n",
    "alpha      = np.power(eig_vals, -2)\n",
    "print(alpha.shape)\n",
    "alpha_norm = (alpha / alpha.sum()) * beta_0\n",
    "t.alpha_   = alpha_norm\n",
    "        \n",
    "print(alpha_norm)\n",
    "\n",
    "t.predict(x_whit,w_mat=True,doc_predict=False)  # normalize the factors \n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = np.array(countvec.get_feature_names())\n",
    "np.save('id_map.npy', id_map)\n",
    "np.save('factors.npy', factors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('trained_tlda.pickle', 'wb') as f:\n",
    "    pickle.dump(t, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_tlda.pickle', 'rb') as f:\n",
    "    t = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1094)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.factors_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = id_map[factors[0,:].argsort()[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top = factors[0,factors[0,:].argsort()[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words=20\n",
    "#print(t_n_indices)\n",
    "n_sentiments = 1\n",
    "top_words_JST = None\n",
    "for k in range(n_topic*n_sentiments):\n",
    "    if k ==0:\n",
    "        t_n_indices   =factors[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_JST = [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]\n",
    "    else:\n",
    "        t_n_indices   =factors[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_JST = np.vstack([top_words_JST, [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]])\n",
    "        # print([i for i,v in countvec.vocabulary_.items() if v in t_n_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(top) == sorted(top_words_JST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['american', 'trump', 'respons', 'handl', 'mask', 'support',\n",
       "        'white', 'cover', 'elect', 'presid', 'call', 'tri', 'vote',\n",
       "        'hous', 'ignor', 'ralli', 'obama', 'administr', 'hoax',\n",
       "        'presid trump'],\n",
       "       ['deal', 'pandem', 'death', 'respons', 'handl', 'caus', 'compar',\n",
       "        'coronavirus pandem', 'pandem http', 'coronavirus death',\n",
       "        'director', 'global', 'global pandem', 'hold', 'worst', 'prepar',\n",
       "        'pandem respons', 'toll', 'death toll', 'predict'],\n",
       "       ['covid', 'like', 'look', 'covid lockdown', 'fight covid',\n",
       "        'protest', 'thing', 'adult', 'amid covid', 'level', 'life',\n",
       "        'depress', 'end', 'look like', 'feel', 'till', 'feel like',\n",
       "        'spread covid', 'respons covid', 'peac'],\n",
       "       ['coronavirus', 'coronavirus http', 'million', 'http coronavirus',\n",
       "        'urg', 'appl', 'drop', 'youtub', 'covid__', 'market',\n",
       "        'coronavirus respons', 'coronavirus crisi', 'die coronavirus',\n",
       "        'coronavirus outbreak', 'victim', 'franc', 'spread coronavirus',\n",
       "        'reveal', 'crisi http', 'million peopl'],\n",
       "       ['virus', 'come', 'virus spread', 'realdonaldtrump', 'virus come',\n",
       "        'fact', 'break', 'believ', 'proof', 'lie', 'wuhan virus', 'tell',\n",
       "        'chines', 'virus http', 'hide', 'wasn', 'yesterday', 'korea',\n",
       "        'south korea', 'januari'],\n",
       "       ['latest', 'updat', 'friday', 'activ', 'recov', 'corona',\n",
       "        'corona virus', 'fight', 'recoveri', 'india', 'amid', 'covid_',\n",
       "        'pandem like', 'nigeria', 'music', 'staysaf', 'stayhom',\n",
       "        'artist', 'trend', 'covid covid'],\n",
       "       ['look', 'recov', 'report', 'time', 'corona', 'corona virus',\n",
       "        'studi', 'african', 'recoveri', 'countri', 'popul', 'total',\n",
       "        'case death', 'mortal', 'singl', 'itali', 'franc', 'canada',\n",
       "        'high', 'fatal'],\n",
       "       ['line', 'risk', 'work', 'time', 'rememb', 'live', 'spend',\n",
       "        'long', 'good', 'import', 'forc', 'job', 'hard', 'love', 'keep',\n",
       "        'listen', 'employ', 'time http', 'class', 'wast'],\n",
       "       ['virus', 'covid virus', 'kill virus', 'patient', 'scienc',\n",
       "        'catch', 'wear', 'mask', 'wear mask', 'studi', 'remdesivir',\n",
       "        'shoot', 'contract', 'virus http', 'fuck', 'face mask', 'shit',\n",
       "        'mutat', 'scar', 'cell'],\n",
       "       ['china', 'china virus', 'wuhan', 'posit coronavirus',\n",
       "        'virus come', 'releas', 'communist', 'purpos',\n",
       "        'coronavirus test', 'travel', 'chines', 'origin', 'investig',\n",
       "        'truth', 'outbreak http', 'russia', 'australia', 'manufactur',\n",
       "        'militari', 'tariff'],\n",
       "       ['need', 'gate', 'million', 'sourc', 'nation', 'number', 'world',\n",
       "        'govern', 'lead', 'countri', 'increas', 'unit', 'unit state',\n",
       "        'economi', 'leader', 'reach', 'massiv', 'truth', 'highest',\n",
       "        'africa'],\n",
       "       ['virus', 'kill', 'peopl', 'die', 'danger', 'spread', 'think',\n",
       "        'spread virus', 'virus spread', 'beach', 'wrong', 'peopl die',\n",
       "        'stupid', 'sick', 'immun', 'fuck', 'understand', 'virus kill',\n",
       "        'sens', 'kill peopl'],\n",
       "       ['deal', 'clear', 'pandem', 'nation', 'say', 'come', 'send',\n",
       "        'pay', 'food', 'billion', 'creat', 'human', 'polit', 'appear',\n",
       "        'massiv', 'give', 'system', 'consid', 'debt', 'say http'],\n",
       "       ['coronavirus', 'warn', 'agenc', 'http http', 'outbreak',\n",
       "        'origin', 'market', 'amid', 'amid coronavirus',\n",
       "        'coronavirus crisi', 'link', 'expert', 'emerg',\n",
       "        'coronavirus origin', 'coronavirus outbreak', 'outbreak http',\n",
       "        'spread coronavirus', 'reveal', 'see evid', 'evid coronavirus'],\n",
       "       ['evid', 'say', 'agenc', 'thursday', 'break', 'south', 'approv',\n",
       "        'news', 'coronavirus trump', 'see', 'suggest', 'news http',\n",
       "        'claim', 'wuhan http', 'trump say', 'see evid',\n",
       "        'evid coronavirus', 'trump claim', 'south korea', 'prime'],\n",
       "       ['open', 'need', 'distanc', 'social distanc', 'wear', 'mask',\n",
       "        'stay', 'safe', 'want', 'governor', 'order', 'protest', 'home',\n",
       "        'stay home', 'face mask', 'nurs home', 'citizen', 'stay safe',\n",
       "        'home order', 'answer'],\n",
       "       ['articl', 'worker', 'mayday', 'organ', 'youtub', 'http youtub',\n",
       "        'coronavirus respons', 'health', 'news http', 'offici',\n",
       "        'health care', 'billgat', 'success', 'mental', 'mental health',\n",
       "        'green', 'guardian', 'public health', 'sweden', 'depart'],\n",
       "       ['conspiraci', 'covid pandem', 'respons', 'middl', 'handl',\n",
       "        'proof', 'fake', 'unit', 'state', 'unit state', 'case covid',\n",
       "        'amid covid', 'vote', 'middl pandem', 'ignor', 'pandem respons',\n",
       "        'emerg', 'covid case', 'declar', 'respons covid'],\n",
       "       ['trump', 'evid', 'case', 'counti', 'see', 'presid', 'total',\n",
       "        'case death', 'case http', 'vote', 'confirm case', 'claim',\n",
       "        'wuhan http', 'donald', 'donald trump', 'see evid',\n",
       "        'evid coronavirus', 'trump claim', 'biden', 'presid trump'],\n",
       "       ['covid', 'covid lockdown', 'read', 'support', 'check', 'group',\n",
       "        'measur', 'chang', 'follow', 'small', 'visit', 'busi',\n",
       "        'amid covid', 'impact', 'local', 'impact covid', 'resourc',\n",
       "        'covid crisi', 'arm', 'lift']], dtype='<U20')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_JST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-07-06 14:38:37.701424\n",
      "(20, 1094)\n",
      "now = 2021-07-06 14:41:13.633491\n",
      "2021-07-06 14:41:13.633880\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "print(\"now =\", now)\n",
    "print(t.factors_.shape)\n",
    "a_word       = tl.tensor(dtm.toarray(),dtype=tl.float32)\n",
    "\n",
    "doc_topic_dist, topic_word_dist = t.predict(a_word,w_mat=False,doc_predict=True)\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "end = datetime.now()\n",
    "print(end)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de8d688998391b4e340423aec176e4bbb9afb78f2320e3ca59b2d8556c4a2b46"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('nlpenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}