{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/adrien/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/adrien/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/adrien/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import gamma\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.cp_tensor import cp_mode_dot\n",
    "import tensorly.tenalg as tnl\n",
    "from tensorly.tenalg.core_tenalg import tensor_dot, batched_tensor_dot, outer, inner\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from pca import PCA\n",
    "\n",
    "# Import TensorLy\n",
    "import tensorly as tl\n",
    "from tensorly.tenalg import kronecker\n",
    "from tensorly import norm\n",
    "from tensorly.decomposition import symmetric_parafac_power_iteration as sym_parafac\n",
    "from tensorly.tenalg.core_tenalg.tensor_product import batched_tensor_dot\n",
    "from tensorly.testing import assert_array_equal, assert_array_almost_equal\n",
    "\n",
    "from tensorly.contrib.sparse.cp_tensor import cp_to_tensor\n",
    "\n",
    "from tlda_final import TLDA\n",
    "import cumulant_gradient\n",
    "import tensor_lda_util as tl_util\n",
    "## Break down into steps, then re-engineer.\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        to_return = [porter.stem(self.wnl.lemmatize(t,get_wordnet_pos(t))) for t in word_tokenize(articles)]\n",
    "        return to_return\n",
    "    \n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.porter.stem(t) for t in word_tokenize(articles)]\n",
    "\n",
    "from helpers import gtp \n",
    "\n",
    "        \n",
    "import gc\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tweets\n",
    "\n",
    "import os\n",
    "from helpers import load_tweets\n",
    "filenames = ['../data/unzipped/' + name for name in os.listdir('../data/unzipped')]\n",
    "\n",
    "all_tweets = load_tweets(filenames, preprocessor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only 100000 tweets\n",
    "tweets = random.sample(all_tweets, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# def stop words\n",
    "n_samples = len(tweets)\n",
    "print(n_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countvec = CountVectorizer(tokenizer=gtp,\n",
    "countvec = CountVectorizer(tokenizer=gtp,\n",
    "                                strip_accents = 'unicode', # works\n",
    "                                lowercase = True, # works\n",
    "                                ngram_range = (1,2),\n",
    "                                max_df = 0.4, # works\n",
    "                                min_df = int(0.002*n_samples))\n",
    "\n",
    "dtm = countvec.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_sent = scipy.sparse.csr_matrix(dtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tl.tensor(dtm_sent.toarray(),dtype=np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dtm_sent\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1      = tl.mean(a, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cent = scipy.sparse.csr_matrix(a - M1,dtype=np.float16) #center the data using the first moment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-07-02 09:07:24.692810\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-086e0eb4ff89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fits PCA to  data, gives W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mx_whit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# produces a whitened words counts <W,x> for centered data x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/twitter-analysis/TensorJST_Public/pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mall\u001b[0m \u001b[0minput\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         '''\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection_weights_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhitening_weights_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/twitter-analysis/nlpenv/lib/python3.8/site-packages/sklearn/decomposition/_incremental_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/twitter-analysis/nlpenv/lib/python3.8/site-packages/sklearn/decomposition/_incremental_pca.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, check_input)\u001b[0m\n\u001b[1;32m    299\u001b[0m                            self.components_, X, mean_correction))\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_flip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_based_decision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mexplained_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_total_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/twitter-analysis/nlpenv/lib/python3.8/site-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0m\u001b[1;32m    126\u001b[0m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "start = datetime.now()\n",
    "print(\"now =\", start)\n",
    "\n",
    "\n",
    "batch_size = int(n_samples/20)\n",
    "verbose = True\n",
    "n_topic =  20\n",
    "\n",
    "beta_0=0.003\n",
    "\n",
    "pca = PCA(n_topic, beta_0, 30000)\n",
    "pca.fit(x_cent) # fits PCA to  data, gives W\n",
    "x_whit = pca.transform(x_cent) # produces a whitened words counts <W,x> for centered data x\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "pca_time = now - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:35.527440\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(pca_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-07-01 10:38:37.511187\n",
      "now = 2021-07-01 10:38:37.512244\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload  \n",
    "import tlda_final\n",
    "reload(tlda_final)\n",
    "from tlda_final import TLDA\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "learning_rate = 0.01 \n",
    "batch_size =15000\n",
    "t = TLDA(n_topic,n_senti=1, alpha_0= beta_0, n_iter_train=1000, n_iter_test=150, batch_size=batch_size,\n",
    "         learning_rate=learning_rate)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-07-01 10:38:37.543647\n",
      "Epoch: 200\n",
      "Epoch: 400\n",
      "Epoch: 600\n",
      "Epoch: 800\n",
      "now = 2021-07-01 10:39:08.668651\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "\n",
    "t.fit(x_whit,verbose=True) # fit whitened wordcounts to get decomposition of M3 through SGD\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-07-01 10:39:08.738405\n",
      "(20,)\n",
      "[2.19424417e-04 1.42362374e-04 1.51911362e-04 6.84582471e-05\n",
      " 1.33031886e-04 2.55738134e-04 2.46100744e-04 2.11430843e-04\n",
      " 1.73731484e-04 1.00908505e-04 1.45674253e-04 7.64351213e-05\n",
      " 7.98640863e-05 2.47342123e-04 8.53057844e-05 1.35044509e-04\n",
      " 2.07739737e-04 7.48047762e-05 1.11046955e-04 1.33644659e-04]\n",
      "now = 2021-07-01 10:39:08.766175\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "\n",
    "\n",
    "t.factors_ = pca.reverse_transform(t.factors_)  # unwhiten the eigenvectors to get unscaled word-level factors\n",
    "\n",
    "''' \n",
    "Recover alpha_hat from the eigenvalues of M3\n",
    "'''  \n",
    "\n",
    "eig_vals = [np.linalg.norm(k,3) for k in t.factors_ ]\n",
    "# normalize beta\n",
    "alpha      = np.power(eig_vals, -2)\n",
    "print(alpha.shape)\n",
    "alpha_norm = (alpha / alpha.sum()) * beta_0\n",
    "t.alpha_   = alpha_norm\n",
    "        \n",
    "print(alpha_norm)\n",
    "\n",
    "t.predict(x_whit,w_mat=True,doc_predict=False)  # normalize the factors \n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'like', 'donald', 'trump', 'mask', 'feel', 'shit', 'covid covid', 'covid coronavirus', 'presid trump', 'covid__', 'coronavirus covid', 'look like', 'act', 'administr', 'obama', 'quarantin', 'sound', 'coronavirus trump', 'feel like']\n",
      "['trump', 'coronavirus', 'donald trump', 'lockdown', 'updat', 'extend', 'michigan', 'rise', 'coronavirus covid', 'lift', 'coronavirus lockdown', 'covid lockdown', 'india', 'case death', 'covid_', 'lockdown http', 'eas', 'coronavirus updat', 'coronavirus live', 'live updat']\n",
      "['long', 'pandem', 'especi', 'help', 'save', 'corona', 'worker', 'work', 'servic', 'fight', 'world', 'happi', 'thank', 'hard', 'abl', 'famili', 'essenti', 'food', 'appreci', 'corona virus']\n",
      "['time', 'infect', 'right', 'go', 'test', 'posit', 'actual', 'number', 'virus go', 'week', 'elonmusk', 'immun', 'antibodi', 'symptom', 'normal', 'negat', 'day', 'time http', 'mean', 'pass']\n",
      "['time', 'right', 'talk', 'mind', 'actual', 'happen', 'global', 'year', 'vaccin', 'lose', 'read', 'articl', 'say', 'truth', 'believ', 'money', 'histori', 'polit', 'listen', 'true']\n",
      "['case', 'today', 'come', 'want', 'know', 'daili', 'happen', 'good', 'good news', 'articl', 'thing', 'love', 'best', 'great', 'person', 'sar', 'know virus', 'morn', 'idea', 'coronavirus case']\n",
      "['total', 'care', 'save', 'updat', 'home', 'daili', 'donat', 'life', 'data', 'live', 'safe', 'mayday', 'continu', 'rais', 'money', 'nurs', 'case death', 'hospit', 'healthi', 'save live']\n",
      "['covid', 'total', 'recov', 'covid pandem', 'updat', 'state', 'covid covid', 'data', 'coronavirus covid', 'confirm', 'confirm case', 'case death', 'http covid', 'counti', 'amid covid', 'track', 'covid updat', 'recoveri', 'record', 'covid virus']\n",
      "['tell', 'want', 'stay', 'home', 'order', 'stay home', 'home order', 'actual', 'year', 'lose', 'report', 'say', 'say http', 'allow', 'believ', 'nurs', 'nurs home', 'ask', 'report http', 'ignor']\n",
      "['time', 'care', 'york', 'risk', 'lose', 'die', 'live', 'bodi', 'black', 'respect', 'love', 'die covid', 'high', 'heart', 'thread', 'nurs', 'hospit', 'time http', 'distanc', 'social distanc']\n",
      "['virus', 'coronavirus', 'natur', 'amid coronavirus', 'http http', 'fake', 'corona', 'coronavirus http', 'coronavirus pandem', 'origin', 'virus http', 'novel', 'know virus', 'virus origin', 'news coronavirus', 'novel coronavirus', 'corona virus', 'coronavirus vaccin', 'virus come', 'cure']\n",
      "['virus', 'spread', 'go', 'away', 'wear', 'mask', 'face mask', 'stop', 'fuck', 'shit', 'think', 'gonna', 'virus go', 'thing', 'mayb', 'realdonaldtrump', 'virus http', 'spread virus', 'virus spread', 'wear mask']\n",
      "['look', 'like', 'presid', 'donald', 'wuhan', 'market', 'research', 'respons', 'origin', 'outbreak', 'look like', 'claim', 'sar', 'drug', 'remdesivir', 'cover', 'intellig', 'treat', 'link', 'comment']\n",
      "['lie', 'china', 'wuhan', 'vaccin', 'coronavirus http', 'communist', 'chines', 'world', 'investig', 'origin', 'outbreak', 'realdonaldtrump', 'creat', 'intellig', 'coronavirus origin', 'evid', 'china http', 'china virus', 'threaten', 'coronavirus patient']\n",
      "['china', 'go', 'lockdown', 'know', 'test', 'posit', 'test posit', 'earli', 'australia', 'travel', 'antibodi', 'coronavirus lockdown', 'negat', 'covid lockdown', 'break', 'lockdown http', 'posit coronavirus', 'coronavirus test', 'posit covid', 'covid test']\n",
      "['look', 'like', 'free', 'test', 'posit', 'test posit', 'employe', 'staff', 'thank', 'target', 'share', 'antibodi', 'look like', 'offer', 'plant', 'posit coronavirus', 'futur', 'coronavirus test', 'posit covid', 'covid test']\n",
      "['time', 'long', 'pandem', 'covid pandem', 'pandem http', 'middl', 'global', 'global pandem', 'year', 'coronavirus pandem', 'report', 'say http', 'world', 'countri', 'expert', 'predict', 'middl pandem', 'cancel', 'report http', 'pandem like']\n",
      "['fall', 'death', 'caus', 'http http', 'feder', 'plan', 'global', 'warn', 'post', 'market', 'hous', 'higher', 'europ', 'toll', 'coronavirus death', 'death toll', 'covid death', 'death http', 'funer', 'death rate']\n",
      "['latest', 'video', 'news', 'face', 'amid', 'amid coronavirus', 'covid pandem', 'updat', 'http http', 'post', 'news http', 'impact', 'fight coronavirus', 'busi', 'affect', 'reveal', 'coronavirus crisi', 'read http', 'nigeria', 'kano']\n"
     ]
    }
   ],
   "source": [
    "n_top_words=20\n",
    "#print(t_n_indices)\n",
    "n_sentiments = 1\n",
    "for k in range(n_topic*n_sentiments):\n",
    "    if k ==0:\n",
    "        t_n_indices   =t.factors_[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_JST = [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]\n",
    "    else:\n",
    "        t_n_indices   =t.factors_[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_JST = np.vstack([top_words_JST, [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]])\n",
    "        print([i for i,v in countvec.vocabulary_.items() if v in t_n_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-07-01 10:12:49.977374\n",
      "(20, 1078)\n",
      "now = 2021-07-01 10:16:43.128848\n",
      "2021-07-01 10:16:43.128974\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "print(\"now =\", now)\n",
    "print(t.factors_.shape)\n",
    "a_word       = tl.tensor(dtm.toarray(),dtype=tl.float32)\n",
    "\n",
    "doc_topic_dist, topic_word_dist = t.predict(a_word,w_mat=False,doc_predict=True)\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "end = datetime.now()\n",
    "print(end)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de8d688998391b4e340423aec176e4bbb9afb78f2320e3ca59b2d8556c4a2b46"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('nlpenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}