{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/adrien/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/adrien/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/adrien/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import gamma\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.cp_tensor import cp_mode_dot\n",
    "import tensorly.tenalg as tnl\n",
    "from tensorly.tenalg.core_tenalg import tensor_dot, batched_tensor_dot, outer, inner\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from pca import PCA\n",
    "\n",
    "# Import TensorLy\n",
    "import tensorly as tl\n",
    "from tensorly.tenalg import kronecker\n",
    "from tensorly import norm\n",
    "from tensorly.decomposition import symmetric_parafac_power_iteration as sym_parafac\n",
    "from tensorly.tenalg.core_tenalg.tensor_product import batched_tensor_dot\n",
    "from tensorly.testing import assert_array_equal, assert_array_almost_equal\n",
    "\n",
    "from tensorly.contrib.sparse.cp_tensor import cp_to_tensor\n",
    "\n",
    "from tlda_final import TLDA\n",
    "import cumulant_gradient\n",
    "import tensor_lda_util as tl_util\n",
    "## Break down into steps, then re-engineer.\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [porter.stem(self.wnl.lemmatize(t,get_wordnet_pos(t))) for t in word_tokenize(articles)]\n",
    "    \n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.porter.stem(t) for t in word_tokenize(articles)]\n",
    "        \n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tweets\n",
    "\n",
    "import os\n",
    "from helpers import load_tweets\n",
    "filenames = ['../data/unzipped/' + name for name in os.listdir('../data/unzipped')]\n",
    "\n",
    "all_tweets = load_tweets(filenames, preprocessor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# use only 100000 tweets\n",
    "tweets = random.sample(all_tweets, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# Import Data and convert to tensor\n",
    "n_samples = len(tweets)\n",
    "print(n_samples)\n",
    "# df         = pd.read_csv(\"../Data/TwitterSpeech.csv\")\n",
    "# df_p       = pd.read_csv(\"../Data/paradigm.csv\")\n",
    "# print(df.head())\n",
    "\n",
    "stop_words = list(stopwords.words('english')) + [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'hi', \"n't\"]\n",
    "# added_words = [#[omitted]]\n",
    "\n",
    "# stop_words = list(np.append(stop_words,added_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |  \n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : {'filename', 'file', 'content'}, default='content'\n",
      " |      - If `'filename'`, the sequence passed as an argument to fit is\n",
      " |        expected to be a list of filenames that need reading to fetch\n",
      " |        the raw content to analyze.\n",
      " |  \n",
      " |      - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      " |        object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      - If `'content'`, the input is expected to be a sequence of items that\n",
      " |        can be of type string or byte.\n",
      " |  \n",
      " |  encoding : string, default='utf-8'\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode'}, default=None\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : bool, default=True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable, default=None\n",
      " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  tokenizer : callable, default=None\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  stop_words : {'english'}, list, default=None\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |      If there is a capturing group in token_pattern then the\n",
      " |      captured group content, not the entire match, becomes the token.\n",
      " |      At most one capturing group is permitted.\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      word n-grams or char n-grams to be extracted. All values of n such\n",
      " |      such that min_n <= n <= max_n will be used. For example an\n",
      " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      " |      Whether the feature should be made of word n-gram or character\n",
      " |      n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |  \n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, default=None\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : bool, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, default=np.int64\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  fixed_vocabulary_: boolean\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = CountVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> print(vectorizer.get_feature_names())\n",
      " |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      " |  >>> print(X.toarray())\n",
      " |  [[0 1 1 1 0 0 1 0 1]\n",
      " |   [0 2 0 1 0 1 1 0 1]\n",
      " |   [1 0 0 1 1 0 1 1 1]\n",
      " |   [0 1 1 1 0 0 1 0 1]]\n",
      " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      " |  >>> print(vectorizer2.get_feature_names())\n",
      " |  ['and this', 'document is', 'first document', 'is the', 'is this',\n",
      " |  'second document', 'the first', 'the second', 'the third', 'third one',\n",
      " |   'this document', 'this is', 'this the']\n",
      " |   >>> print(X2.toarray())\n",
      " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return document-term matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list\n",
      " |          A list of feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays of shape (n_samples,)\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing, tokenization\n",
      " |      and n-grams generation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : str\n",
      " |          The string to decode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrien/twitter-analysis/nlpenv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'might', 'must', 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "## Pre-process Data\n",
    "# print(int(0.002*n_samples))\n",
    "countvec = CountVectorizer(tokenizer=StemTokenizer(),\n",
    "                                strip_accents = 'unicode', # works \n",
    "                                stop_words = stop_words, # works\n",
    "                                lowercase = True, # works\n",
    "                                ngram_range = (1,2),\n",
    "                                max_df = 0.4, # works\n",
    "                                min_df = int(0.002*n_samples))\n",
    "\n",
    "dtm = countvec.fit_transform(tweets)\n",
    "    # df.tweet[ df.year>=2019][:n_samples])\n",
    "\n",
    "# print(dtm.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # find to sentiments\n",
    "    # sum_words = dtm.sum(axis=0) \n",
    "    # words_freq = [(word, sum_words[0, idx]) for word, idx in     countvec.vocabulary_.items()]\n",
    "    # words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    # print(words_freq[:100])\n",
    "    \n",
    "    # top_sents = [ i[0] for i in  words_freq[:1000] if i[0] in df_p[\"Token\"].unique() ]\n",
    "    # print(top_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm_pos = np.float_(dtm.toarray())\n",
    "# dtm_neu = np.float_(dtm.toarray())\n",
    "# dtm_neg = np.float_(dtm.toarray())\n",
    "# for i,v in countvec.vocabulary_.items():\n",
    "\n",
    "#         if i in df_p[\"Token\"].unique() and i in top_sents[:100]:\n",
    "#             print(i)\n",
    "#             print(df_p.Positive[df_p.Token==i])\n",
    "#             dtm_pos[:,v] *= df_p.Positive[df_p.Token==i].unique() \n",
    "#             dtm_neg[:,v] *= df_p.Negative[df_p.Token==i].unique() \n",
    "#             dtm_neu[:,v] *= df_p.Neutral[df_p.Token==i].unique()\n",
    "#         else:\n",
    "#             dtm_pos[:,v] *= 1/3\n",
    "#             dtm_neg[:,v] *= 1/3\n",
    "#             dtm_neu[:,v] *= 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm_sent = scipy.sparse.csr_matrix(np.concatenate((dtm_pos,dtm_neg,dtm_neu),axis=1))\n",
    "dtm_sent = scipy.sparse.csr_matrix(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dtm_pos,dtm_neg,dtm_neu\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tl.tensor(dtm_sent.toarray(),dtype=np.float16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537323"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dtm_sent\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M1      = tl.mean(a, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_cent = scipy.sparse.csr_matrix(a - M1,dtype=np.float16) #center the data using the first moment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-06-30 14:57:23.932368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-9e0f7371840d>:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  batch_size = np.int(n_samples/20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-06-30 14:58:36.233027\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = datetime.now()\n",
    "print(\"now =\", start)\n",
    "\n",
    "\n",
    "batch_size = int(n_samples/20)\n",
    "verbose = True\n",
    "n_topic =  20\n",
    "\n",
    "beta_0=0.003\n",
    "\n",
    "pca = PCA(n_topic, beta_0, 30000)\n",
    "pca.fit(x_cent) # fits PCA to  data, gives W\n",
    "x_whit = pca.transform(x_cent) # produces a whitened words counts <W,x> for centered data x\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "pca_time = now - start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:12.300659\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "print(pca_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-06-30 15:00:43.186342\n",
      "now = 2021-06-30 15:00:43.186561\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload  \n",
    "import tlda_final\n",
    "reload(tlda_final)\n",
    "from tlda_final import TLDA\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "learning_rate = 0.01 \n",
    "batch_size =15000\n",
    "t = TLDA(n_topic,n_senti=1, alpha_0= beta_0, n_iter_train=1000, n_iter_test=150, batch_size=batch_size,\n",
    "         learning_rate=learning_rate)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-06-30 15:00:46.878562\n",
      "Epoch: 200\n",
      "Epoch: 400\n",
      "Epoch: 600\n",
      "Epoch: 800\n",
      "now = 2021-06-30 15:01:07.775973\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "\n",
    "t.fit(x_whit,verbose=True) # fit whitened wordcounts to get decomposition of M3 through SGD\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-06-30 15:01:36.934330\n",
      "(20,)\n",
      "[2.55220743e-04 1.10839107e-04 2.17597749e-04 1.03870983e-04\n",
      " 6.83356304e-05 6.48767916e-05 1.63810679e-04 1.99996236e-04\n",
      " 1.13083539e-04 1.22295385e-04 9.84124680e-05 1.02496042e-04\n",
      " 9.32648939e-05 1.37232622e-04 2.88174135e-04 2.53707756e-04\n",
      " 6.28712085e-05 1.81468463e-04 9.22223774e-05 2.70223192e-04]\n",
      "now = 2021-06-30 15:01:36.946814\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "\n",
    "\n",
    "t.factors_ = pca.reverse_transform(t.factors_)  # unwhiten the eigenvectors to get unscaled word-level factors\n",
    "\n",
    "''' \n",
    "Recover alpha_hat from the eigenvalues of M3\n",
    "'''  \n",
    "\n",
    "eig_vals = [np.linalg.norm(k,3) for k in t.factors_ ]\n",
    "# normalize beta\n",
    "alpha      = np.power(eig_vals, -2)\n",
    "print(alpha.shape)\n",
    "alpha_norm = (alpha / alpha.sum()) * beta_0\n",
    "t.alpha_   = alpha_norm\n",
    "        \n",
    "print(alpha_norm)\n",
    "\n",
    "t.predict(x_whit,w_mat=True,doc_predict=False)  # normalize the factors \n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ????\n",
    "# factors= t.factors_\n",
    "# print((factors.shape))\n",
    "# factors_reshape = np.concatenate((factors[:,0:(factors.shape[1]//3)],\n",
    "#                                   factors[:,(factors.shape[1]//3):(2*factors.shape[1]//3)],\n",
    "#                                   factors[:,(2*factors.shape[1]//3):(factors.shape[1])]),axis=0)\n",
    "# factors_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.factors_ = factors_reshape\n",
    "# #t.factors_ = factors\n",
    "# now = datetime.now()\n",
    " \n",
    "# print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'commun', 'partner', 'inform', 'region', 'gt', 'gt ;', ') ,', ', wa', 'public', 'accord', 'high', ', &', 'worker ,', 'includ', ', includ', 'level', 'sars-cov-2', ', may', 'among']\n",
      "['http http', 'respons', 'covid-19 pandem', 'coronaviru pandem', '. http', 'impact', '–', 'emerg', 'author', 'crisi', 'demand', 'offici', 'amid', 'pandem http', '|', 'amid covid-19', 'ceo', 'u.s.', 'two', 'dure covid-19']\n",
      "['dure thi', 'thi pandem', 'thank', 'educ', 'worker', 'celebr', 'end', 'frontlin', 'thi #', 'love', 'opportun', \"'m\", 'happi', 'special', 'especi', 'labour', 'salut', 'surviv', 'thi time', 'intern']\n",
      "['peopl', '. peopl', ', peopl', 'care', 'risk', 'help', 'protect', 'home', 'die', 'peopl die', 'wear', 'stay home', 'social', 'distanc', 'food', 'back work', 'peopl .', 'social distanc', 'peopl ,', 'mani peopl']\n",
      "['?', 'dure', 'pandem', 'dure thi', 'thi pandem', '(', ')', 'global', 'pandem ?', '? http', ') http', 'pandem .', 'global pandem', '? ?', 'dure pandem', '? #', 'middl', 'pandem #', '# pandem', '. (']\n",
      "['covid-19', 'covid-19 http', 'thi covid-19', 'due covid-19', 'patient', 'covid-19 patient', 'fda', 'remdesivir', 'covid-19 lockdown', 'relief', 'covid-19 vaccin', 'amid covid-19', 'dure covid-19', 'sign', ': covid-19', 'treatment', 'fight covid-19', 'covid-19 crisi', 'covid-19 outbreak', 'impact covid-19']\n",
      "[':', 'news :', ': http', 'reopen', 'updat :', 'coronaviru :', 'coronaviru .', 'rt', ': #', 'top', 'link', ': coronaviru', 'read :', 'restrict', 'live updat', 'lift', 'china .', 'countri .', '. read', ': @']\n",
      "['-', 'pandem -', '- http', '@', 'covid-19 -', 'news', '\\u2066', '\\u2066 @', 'bbc', 'bbc news', 'news -', '- coronaviru', 'gt', '& gt', 'gt ;', 'time http', 'youtub', 'sign', 'opinion', 'un']\n",
      "['peopl', '. peopl', ', peopl', 'care', 'area', 'data', 'home', 'peopl die', 'activ', 'visit', 'new case', 'daili', 'doubl', 'thread', 'recov', 'death :', 'peopl .', 'peopl ,', 'black', 'yesterday']\n",
      "['-', '- http', '!', '! http', '! !', 'bbc', 'bbc news', 'news -', '- coronaviru', 'coronaviru test', 'time http', 'coronaviru lockdown', 'wow', 'posit coronaviru', '? !', '! ?', ', coronaviru', 'due coronaviru', 'coronaviru ?', 'british']\n",
      "['-', ', one', 'covid-19 -', 'nigeria', 'death ,', 'increas', 'kano', 'new case', 'texa', 'record', ', new', 'new covid-19', 'addit', 'counti', 'covid-19 case', 'march', 'case ,', ', ,', 'case covid-19', 'georgia']\n",
      "['#', 'china', 'stock', '# china', '# health', '# trump', 'australia', 'health #', 'trade', '? #', '# news', 'news #', 'ban', 'sale', '; #', 'travel', 'china #', '# india', 'fit', 'foxnew']\n",
      "['viru', 'viru !', 'corona viru', 'viru .', 'realdonaldtrump', '@ realdonaldtrump', '@ cnn', '. viru', 'viru ?', '....', 'ye', 'yeah', 'lol', 'sweden', 'wow', 'spread viru', 'sar', 'exactli', 'bat', 'contagi']\n",
      "['. http', 'http @', ', @', 'crisi', 'crisi .', 'pandem ’', 'discuss', '. @', '``', \"''\", 'thank @', 'piec', \"'' http\", 'colleg', \". ''\", \", ''\", \"'' #\", '. ``', ': ``', \"'' .\"]\n",
      "['peopl', '. peopl', ', peopl', 'home', 'peopl die', 'wear', 'stay', 'lockdown', 'stay home', 'mask', 'store', 'protest', 'back work', 'peopl .', 'million peopl', 'face mask', 'shop', 'peopl ,', 'wear mask', 'mani peopl']\n",
      "['!', '! http', 'viru !', '! !', 'bill', 'bill gate', 'via', 'http via', 'via @', 'amid coronaviru', 'task', 'youtub', '@ youtub', 'sign', '! ’', 'vote', '’ coronaviru', 'pandem !', '! @', 'block']\n",
      "['http http', ':', '2020', 'updat', 'coronaviru updat', 'updat :', 'covid19', '# covid19', 'extend', 'covid19 #', 'pm', 'india', '|', 'survey', 'latest', 'recoveri', 'recov', 'death :', ', 2020', 'zone']\n",
      "['dure', 'pandem', 'covid-19 pandem', 'global', 'dure #', 'pandem .', 'global pandem', 'covid19', '# covid19', 'servic', 'pandem :', 'student', 'dure pandem', 'middl', 'pandem #', '# pandem', 'labour', 'blog', 'covid19 pandem', 'pandem !']\n",
      "['’', '. ’', 'case', 'today', 'pandem ’', 'count', 'new', '‘', ', ’', 'new case', 'middl', 'let ’', 'case .', 'covid-19 case', '’ covid-19', 'today .', 'middl pandem', '’ .', '“ ’', '’ get']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_top_words=20\n",
    "#print(t_n_indices)\n",
    "n_sentiments = 1\n",
    "for k in range(n_topic*n_sentiments):\n",
    "    if k ==0:\n",
    "        t_n_indices   =t.factors_[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_JST = [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]\n",
    "    else:\n",
    "        t_n_indices   =t.factors_[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_JST = np.vstack([top_words_JST, [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]])\n",
    "        print([i for i,v in countvec.vocabulary_.items() if v in t_n_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-06-30 15:04:54.312429\n",
      "(20, 1615)\n",
      "now = 2021-06-30 15:11:13.641607\n",
      "2021-06-30 15:11:13.642524\n"
     ]
    }
   ],
   "source": [
    "\n",
    "now = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "print(\"now =\", now)\n",
    "print(t.factors_.shape)\n",
    "a_word       = tl.tensor(dtm.toarray(),dtype=tl.float32)\n",
    "\n",
    "doc_topic_dist, topic_word_dist = t.predict(a_word,w_mat=False,doc_predict=True)\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "end = datetime.now()\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hate' in countvec.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec.vocabulary_['hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10080967, 0.04867432, 0.00697167, 0.1324299 , 0.13905791,\n",
       "       0.04894179, 0.01547933, 0.01335646, 0.0314051 , 0.08771742,\n",
       "       0.08749572, 0.00697167, 0.00697167, 0.08518365, 0.00697167,\n",
       "       0.07363147, 0.06081808, 0.00697167, 0.02606205, 0.01407878])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_dist[:,768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.42159261e-11, 5.42159226e-11, 5.42159347e-11, 6.97614838e-01,\n",
       "       5.42159606e-11, 5.42227882e-11, 5.42159085e-11, 5.42159523e-11,\n",
       "       5.42159350e-11, 3.02385161e-01, 5.42159110e-11, 5.42159065e-11,\n",
       "       5.42159172e-11, 5.42159199e-11, 5.42159289e-11, 5.42159211e-11,\n",
       "       5.42159153e-11, 5.42159514e-11, 5.42159526e-11, 5.42159204e-11])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_dist[8081]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The @ONS is launching two online surveys as the organisation steps up its efforts to gather enough #data to help it understand the effects of the #coronavirus on society and the economy.\\nRead more: http'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[8081]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_sparse = scipy.sparse.csc_matrix(dtm.toarray(),dtype=np.float16)\n",
    "dtm_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_gensim = datetime.now()\n",
    "lda = LatentDirichletAllocation(n_components=84,n_jobs=-1,\n",
    "                                learning_method=\"online\",verbose=1,max_iter=1000,\n",
    "                                evaluate_every=10,batch_size=15000,max_doc_update_iter=150,perp_tol=1e-3)\n",
    "lda.fit(dtm)\n",
    "\n",
    "\n",
    "end_1 = datetime.now()\n",
    "\n",
    "doc_topic_LDA = lda.transform(dtm)\n",
    "end_2 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end-start)\n",
    "print(end_2-start_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words=20\n",
    "#print(t_n_indices)\n",
    "\n",
    "for k in range(n_topic*3):\n",
    "    if k ==0:\n",
    "        t_n_indices   =lda.components_[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_LDA = [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]\n",
    "    else:\n",
    "        t_n_indices   = lda.components_[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_LDA = np.vstack([top_words_JST, [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_words_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(doc_topic_LDA).to_csv(\"../Data/theta_LDA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(doc_topic_dist).to_csv(\"../Data/theta_JST_Tensor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_words_LDA).to_csv(\"../Data/theta_LDA_TopWords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_words_JST).to_csv(\"../Data/theta_JST_TopWords.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de8d688998391b4e340423aec176e4bbb9afb78f2320e3ca59b2d8556c4a2b46"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nlpenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}